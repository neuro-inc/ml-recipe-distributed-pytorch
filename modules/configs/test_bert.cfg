# model
model=bert-base-uncased

vocab_file=./data/bert-base-uncased-vocab.txt
merges_file=None

lowercase=True
handle_chinese_chars=False

hidden_dropout_prob=0.1
attention_probs_dropout_prob=0.1

# trainer
dump_dir=./results
experiment_name=test
last=None

gpu=True

seed=None

n_jobs=128
n_epochs=2

train_batch_size=256
test_batch_size=16
batch_split=128

w_start=1
w_end=1
w_start_reg=1
w_end_reg=1
w_cls=1

loss = smooth

smooth_alpha = 0.01

focal_alpha=1
focal_gamma=2

warmup_coef=0.6
apex_level=O1
apex_verbosity=0

lr=1e-5
weight_decay=1e-4

max_grad_norm=1
sync_bn=True

data_path=./data/simplified-nq-train.jsonl
processed_data_path=./data/processed
clear_processed=False

drop_optimizer=True

best_metric=map
best_order=>

finetune=False
finetune_transformer=False
finetune_position=False
finetune_class=False

max_seq_len=512
max_question_len=64
doc_stride=15

split_by_sentence=True
truncate=True

train_label_weights=True
train_sampler_weights=True

debug=True
dummy_dataset=True
